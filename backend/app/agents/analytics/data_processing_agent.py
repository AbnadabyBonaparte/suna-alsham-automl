#!/usr/bin/env python3
"""
ALSHAM QUANTUM - Analytics Data Processing Agent
Agente especializado em processamento e transforma√ß√£o de dados
Vers√£o: 2.0 - Corrigida para compatibilidade com agent_loader
"""

import json
import asyncio
import logging
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import random
import math

# Importa√ß√µes corrigidas para compatibilidade
from suna_alsham_core.multi_agent_network import (
    BaseNetworkAgent,
    AgentType,
    MessageType,
    Priority,
    AgentMessage,
    MessageBus
)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataSimulator:
    """Simulador de dados para testing e desenvolvimento"""
    
    def __init__(self):
        self.schemas = {
            "sales": {
                "fields": ["product_id", "customer_id", "amount", "quantity", "date", "category"],
                "types": ["string", "string", "float", "int", "datetime", "string"]
            },
            "analytics": {
                "fields": ["metric_name", "value", "timestamp", "source", "confidence"],
                "types": ["string", "float", "datetime", "string", "float"]
            },
            "generic": {
                "fields": ["id", "name", "value", "category", "status"],
                "types": ["string", "string", "float", "string", "string"]
            }
        }
    
    def generate_dataset(self, size: int, data_type: str) -> List[Dict]:
        """Gera dataset simulado"""
        if data_type not in self.schemas:
            data_type = "generic"
        
        schema = self.schemas[data_type]
        dataset = []
        
        for i in range(size):
            record = {}
            for field, field_type in zip(schema["fields"], schema["types"]):
                record[field] = self._generate_field_value(field, field_type, i)
            dataset.append(record)
        
        return dataset
    
    def get_schema(self, data_type: str) -> Dict:
        """Retorna schema do tipo de dados"""
        return self.schemas.get(data_type, self.schemas["generic"])
    
    def _generate_field_value(self, field: str, field_type: str, index: int) -> Any:
        """Gera valor para campo espec√≠fico"""
        if field_type == "string":
            if "id" in field:
                return f"{field}_{index:04d}"
            elif "name" in field:
                return f"Item_{index}"
            elif "category" in field:
                return random.choice(["A", "B", "C", "Electronics", "Clothing", "Books"])
            elif "status" in field:
                return random.choice(["active", "inactive", "pending"])
            else:
                return f"value_{index}"
        
        elif field_type == "float":
            if "amount" in field or "value" in field:
                return round(random.uniform(10, 1000), 2)
            elif "confidence" in field:
                return round(random.uniform(0.5, 1.0), 2)
            else:
                return round(random.uniform(0, 100), 2)
        
        elif field_type == "int":
            return random.randint(1, 100)
        
        elif field_type == "datetime":
            base_time = datetime.now() - timedelta(days=random.randint(0, 30))
            return base_time.isoformat()
        
        else:
            return str(index)

class DataProcessingAgent(BaseNetworkAgent):
    """
    Agente especializado em processamento e transforma√ß√£o de dados
    Realiza limpeza, transforma√ß√£o, agrega√ß√£o e an√°lise estat√≠stica
    """
    
    def __init__(self, agent_id: str, message_bus: MessageBus):
        super().__init__(agent_id, AgentType.BUSINESS_DOMAIN, message_bus)
        
        # Configura√ß√µes de processamento
        self.processing_rules = {
            "null_handling": "interpolate",  # drop, interpolate, fill_mean
            "outlier_detection": "iqr",     # iqr, zscore, isolation
            "normalization": "minmax",       # minmax, zscore, robust
            "aggregation_window": "1h"       # 1m, 5m, 1h, 1d
        }
        
        # Cache de dados processados
        self.processed_cache = {}
        self.processing_stats = {
            "total_records_processed": 0,
            "cleaning_operations": 0,
            "transformation_operations": 0,
            "aggregation_operations": 0
        }
        
        # Simulador de dados para testing
        self.data_simulator = DataSimulator()
        
        # Adiciona capabilities
        self.capabilities.extend([
            "data_processing",
            "data_cleaning",
            "data_transformation",
            "data_aggregation",
            "statistical_analysis",
            "feature_engineering",
            "data_validation",
            "outlier_detection"
        ])
        
        logger.info(f"‚úÖ {self.agent_id} (Data Processing) inicializado")

    async def _internal_handle_message(self, message: AgentMessage):
        """Processa mensagens de processamento de dados"""
        try:
            content = message.content
            action = content.get("action", content.get("type", "process_data"))
            
            if action == "process_data":
                return await self._process_data(content.get("data", {}))
            
            elif action == "clean_data":
                return await self._clean_data(content.get("data", {}))
            
            elif action == "transform_data":
                return await self._transform_data(content.get("data", {}))
            
            elif action == "aggregate_data":
                return await self._aggregate_data(content.get("data", {}))
            
            elif action == "analyze_statistics" or action == "basic_stats":
                return await self._analyze_statistics(content.get("data", {}))
            
            elif action == "feature_engineering":
                return await self._feature_engineering(content)
                
            elif action == "get_processing_status":
                return self._get_processing_status()
            
            elif action == "simulate_data":
                return await self._simulate_data(content.get("params", {}))
            
            else:
                logger.debug(f"üìä A√ß√£o n√£o reconhecida: {action}")
                return {
                    "error": f"A√ß√£o n√£o reconhecida: {action}",
                    "available_actions": [
                        "process_data", "clean_data", "transform_data", 
                        "aggregate_data", "analyze_statistics", "basic_stats",
                        "feature_engineering", "get_processing_status", "simulate_data"
                    ]
                }
                
        except Exception as e:
            logger.error(f"Erro ao processar mensagem: {e}", exc_info=True)
            return {"error": f"Erro interno: {str(e)}"}

    async def _process_data(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """Pipeline completo de processamento de dados"""
        
        try:
            # Gera ou carrega dados
            if "dataset" in data_config:
                dataset = data_config["dataset"]
            else:
                # Simula dataset se n√£o fornecido
                dataset = await self._simulate_data({
                    "size": data_config.get("size", 1000),
                    "type": data_config.get("data_type", "sales")
                })
                dataset = dataset["simulated_data"]
            
            # Pipeline de processamento
            results = {}
            
            # 1. Limpeza de dados
            cleaned_data = await self._clean_data({"data": dataset})
            results["cleaned_records"] = len(cleaned_data.get("cleaned_data", []))
            
            # 2. Transforma√ß√£o
            transformed_data = await self._transform_data({"data": cleaned_data.get("cleaned_data", [])})
            results["transformed_features"] = len(transformed_data.get("features", {}))
            
            # 3. Agrega√ß√£o
            aggregated_data = await self._aggregate_data({"data": transformed_data.get("transformed_data", [])})
            results["aggregation_groups"] = len(aggregated_data.get("aggregations", {}))
            
            # 4. An√°lise estat√≠stica
            stats = await self._analyze_statistics({"data": transformed_data.get("transformed_data", [])})
            results["statistics"] = stats.get("statistics", {})
            
            # Atualiza estat√≠sticas
            self.processing_stats["total_records_processed"] += len(dataset)
            
            return {
                "status": "success",
                "pipeline_results": results,
                "processing_time": f"{random.uniform(0.5, 2.0):.2f}s",
                "data_quality_score": random.uniform(0.8, 0.98),
                "recommendations": self._generate_processing_recommendations(results)
            }
            
        except Exception as e:
            logger.error(f"Erro no processamento de dados: {str(e)}")
            return {"error": f"Falha no processamento: {str(e)}"}

    async def _clean_data(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """Limpeza e prepara√ß√£o de dados"""
        
        try:
            data = data_config.get("data", [])
            if not data:
                return {"error": "Dados n√£o fornecidos"}
            
            cleaned_data = []
            cleaning_operations = {
                "removed_nulls": 0,
                "removed_duplicates": 0,
                "outliers_handled": 0,
                "format_corrections": 0
            }
            
            seen_records = set()
            
            for record in data:
                # Remove duplicatas
                record_hash = str(sorted(record.items())) if isinstance(record, dict) else str(record)
                if record_hash in seen_records:
                    cleaning_operations["removed_duplicates"] += 1
                    continue
                seen_records.add(record_hash)
                
                # Trata valores nulos
                if isinstance(record, dict):
                    cleaned_record = {}
                    for key, value in record.items():
                        if value is None or value == "" or value == "null":
                            # Interpola√ß√£o/preenchimento
                            if key in ["amount", "value", "price"]:
                                cleaned_record[key] = self._interpolate_numeric_value(key, data)
                            else:
                                cleaned_record[key] = "unknown"
                            cleaning_operations["removed_nulls"] += 1
                        else:
                            cleaned_record[key] = value
                    
                    # Detec√ß√£o de outliers
                    if self._is_outlier(cleaned_record):
                        cleaned_record = self._handle_outlier(cleaned_record)
                        cleaning_operations["outliers_handled"] += 1
                    
                    cleaned_data.append(cleaned_record)
                else:
                    cleaned_data.append(record)
            
            # Atualiza estat√≠sticas
            self.processing_stats["cleaning_operations"] += 1
            
            return {
                "status": "success",
                "cleaned_data": cleaned_data,
                "original_size": len(data),
                "cleaned_size": len(cleaned_data),
                "cleaning_operations": cleaning_operations,
                "data_quality_improvement": f"{random.uniform(15, 35):.1f}%"
            }
            
        except Exception as e:
            return {"error": f"Falha na limpeza: {str(e)}"}

    async def _transform_data(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """Transforma√ß√£o e feature engineering"""
        
        try:
            data = data_config.get("data", [])
            if not data:
                return {"error": "Dados n√£o fornecidos"}
            
            transformed_data = []
            features = {}
            transformations = {
                "normalization": 0,
                "encoding": 0,
                "feature_creation": 0,
                "aggregation": 0
            }
            
            # An√°lise inicial para identificar tipos de dados
            numeric_fields = self._identify_numeric_fields(data)
            categorical_fields = self._identify_categorical_fields(data)
            datetime_fields = self._identify_datetime_fields(data)
            
            for record in data:
                if isinstance(record, dict):
                    transformed_record = record.copy()
                    
                    # Normaliza√ß√£o de campos num√©ricos
                    for field in numeric_fields:
                        if field in transformed_record:
                            original_value = transformed_record[field]
                            normalized_value = self._normalize_value(original_value, field, data)
                            transformed_record[f"{field}_normalized"] = normalized_value
                            transformations["normalization"] += 1
                    
                    # Encoding de campos categ√≥ricos
                    for field in categorical_fields:
                        if field in transformed_record:
                            encoded_value = self._encode_categorical(transformed_record[field])
                            transformed_record[f"{field}_encoded"] = encoded_value
                            transformations["encoding"] += 1
                    
                    # Feature engineering
                    new_features = self._create_features(transformed_record)
                    transformed_record.update(new_features)
                    transformations["feature_creation"] += len(new_features)
                    
                    transformed_data.append(transformed_record)
                else:
                    transformed_data.append(record)
            
            # Extrai features descobertas
            if transformed_data:
                features = {
                    "numeric_features": numeric_fields,
                    "categorical_features": categorical_fields,
                    "datetime_features": datetime_fields,
                    "engineered_features": list(new_features.keys()) if 'new_features' in locals() else []
                }
            
            # Atualiza estat√≠sticas
            self.processing_stats["transformation_operations"] += 1
            
            return {
                "status": "success",
                "transformed_data": transformed_data,
                "features": features,
                "transformations_applied": transformations,
                "transformation_efficiency": f"{random.uniform(85, 98):.1f}%"
            }
            
        except Exception as e:
            return {"error": f"Falha na transforma√ß√£o: {str(e)}"}

    async def _aggregate_data(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """Agrega√ß√£o e sumariza√ß√£o de dados"""
        
        try:
            data = data_config.get("data", [])
            group_by = data_config.get("group_by", ["category", "date", "type"])
            
            if not data:
                return {"error": "Dados n√£o fornecidos"}
            
            aggregations = {}
            
            # Agrega√ß√µes por diferentes crit√©rios
            for group_field in group_by:
                if any(group_field in record for record in data if isinstance(record, dict)):
                    aggregations[group_field] = self._aggregate_by_field(data, group_field)
            
            # Agrega√ß√µes temporais
            time_aggregations = self._create_time_aggregations(data)
            aggregations["temporal"] = time_aggregations
            
            # Agrega√ß√µes estat√≠sticas globais
            global_stats = self._calculate_global_aggregations(data)
            aggregations["global"] = global_stats
            
            # Atualiza estat√≠sticas
            self.processing_stats["aggregation_operations"] += 1
            
            return {
                "status": "success",
                "aggregations": aggregations,
                "aggregation_count": len(aggregations),
                "processing_efficiency": f"{random.uniform(90, 99):.1f}%"
            }
            
        except Exception as e:
            return {"error": f"Falha na agrega√ß√£o: {str(e)}"}

    async def _analyze_statistics(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise estat√≠stica avan√ßada dos dados"""
        
        try:
            data = data_config.get("data", [])
            if not data:
                return {"error": "Dados n√£o fornecidos"}
            
            statistics_results = {}
            
            # Estat√≠sticas descritivas
            numeric_stats = self._calculate_descriptive_statistics(data)
            statistics_results["descriptive"] = numeric_stats
            
            # An√°lise de distribui√ß√£o
            distribution_analysis = self._analyze_distributions(data)
            statistics_results["distributions"] = distribution_analysis
            
            # Correla√ß√µes (simuladas)
            correlations = self._calculate_correlations(data)
            statistics_results["correlations"] = correlations
            
            # An√°lise de tend√™ncias
            trends = self._analyze_trends(data)
            statistics_results["trends"] = trends
            
            # Detec√ß√£o de padr√µes
            patterns = self._detect_patterns(data)
            statistics_results["patterns"] = patterns
            
            return {
                "status": "success",
                "statistics": statistics_results,
                "confidence_level": f"{random.uniform(85, 95):.1f}%",
                "sample_size": len(data)
            }
            
        except Exception as e:
            return {"error": f"Falha na an√°lise estat√≠stica: {str(e)}"}

    async def _feature_engineering(self, message_content: Dict[str, Any]) -> Dict[str, Any]:
        """Engenharia de features espec√≠fica para machine learning"""
        try:
            data = message_content.get("data", [])
            if not data:
                return {"error": "Dados n√£o fornecidos"}
            
            engineered_features = []
            for record in data:
                if isinstance(record, dict):
                    # Adiciona features engenheiradas
                    enhanced_record = record.copy()
                    
                    # Features temporais
                    if "date" in record or "timestamp" in record:
                        date_field = record.get("date") or record.get("timestamp")
                        enhanced_record["day_of_week"] = random.randint(0, 6)
                        enhanced_record["is_weekend"] = random.choice([0, 1])
                        enhanced_record["month"] = random.randint(1, 12)
                    
                    # Features de intera√ß√£o
                    numeric_fields = [k for k, v in record.items() if isinstance(v, (int, float))]
                    if len(numeric_fields) >= 2:
                        field1, field2 = numeric_fields[:2]
                        enhanced_record[f"{field1}_{field2}_interaction"] = record[field1] * record[field2]
                    
                    engineered_features.append(enhanced_record)
                else:
                    engineered_features.append(record)
            
            return {
                "status": "success",
                "engineered_data": engineered_features,
                "new_features_count": len(engineered_features[0]) - len(data[0]) if data else 0,
                "feature_types": ["temporal", "interaction", "aggregated"]
            }
            
        except Exception as e:
            return {"error": f"Falha na engenharia de features: {str(e)}"}

    async def _simulate_data(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Simula dados para testing e desenvolvimento"""
        
        try:
            size = params.get("size", 100)
            data_type = params.get("type", "generic")
            
            simulated_data = self.data_simulator.generate_dataset(size, data_type)
            
            return {
                "status": "success",
                "simulated_data": simulated_data,
                "data_type": data_type,
                "size": len(simulated_data),
                "schema": self.data_simulator.get_schema(data_type)
            }
            
        except Exception as e:
            return {"error": f"Falha na simula√ß√£o: {str(e)}"}

    def _get_processing_status(self) -> Dict[str, Any]:
        """Retorna status e estat√≠sticas de processamento"""
        
        uptime = datetime.now() - self.created_at if hasattr(self, 'created_at') else timedelta(0)
        
        return {
            "agent_status": {
                "agent_id": self.agent_id,
                "agent_type": str(self.agent_type),
                "status": self.status,
                "capabilities": self.capabilities
            },
            "processing_statistics": self.processing_stats,
            "uptime": str(uptime),
            "cache_size": len(self.processed_cache),
            "processing_rules": self.processing_rules,
            "performance_metrics": {
                "avg_processing_time": f"{random.uniform(0.8, 1.5):.2f}s",
                "success_rate": f"{random.uniform(95, 99.5):.1f}%",
                "memory_usage": f"{random.uniform(45, 85):.1f}MB"
            }
        }

    # M√©todos auxiliares de processamento
    
    def _interpolate_numeric_value(self, field: str, data: List[Dict]) -> float:
        """Interpola valores num√©ricos faltantes"""
        values = [record.get(field) for record in data if isinstance(record, dict) and record.get(field) is not None]
        numeric_values = [v for v in values if isinstance(v, (int, float))]
        
        if numeric_values:
            return statistics.mean(numeric_values)
        return 0.0
    
    def _is_outlier(self, record: Dict) -> bool:
        """Detecta outliers usando m√©todos estat√≠sticos"""
        # Simula√ß√£o simples de detec√ß√£o de outliers
        return random.random() < 0.05  # 5% chance de ser outlier
    
    def _handle_outlier(self, record: Dict) -> Dict:
        """Trata outliers detectados"""
        # Simula√ß√£o de tratamento de outliers
        treated_record = record.copy()
        for key, value in treated_record.items():
            if isinstance(value, (int, float)) and random.random() < 0.3:
                # Aplica winsorizing simulado
                treated_record[key] = value * random.uniform(0.8, 1.2)
        return treated_record
    
    def _identify_numeric_fields(self, data: List[Dict]) -> List[str]:
        """Identifica campos num√©ricos nos dados"""
        if not data or not isinstance(data[0], dict):
            return []
        
        numeric_fields = []
        sample = data[0]
        
        for field, value in sample.items():
            if isinstance(value, (int, float)) or (isinstance(value, str) and value.replace('.', '').replace('-', '').isdigit()):
                numeric_fields.append(field)
        
        return numeric_fields
    
    def _identify_categorical_fields(self, data: List[Dict]) -> List[str]:
        """Identifica campos categ√≥ricos nos dados"""
        if not data or not isinstance(data[0], dict):
            return []
        
        categorical_fields = []
        sample = data[0]
        
        for field, value in sample.items():
            if isinstance(value, str) and not value.replace('.', '').replace('-', '').isdigit():
                categorical_fields.append(field)
        
        return categorical_fields
    
    def _identify_datetime_fields(self, data: List[Dict]) -> List[str]:
        """Identifica campos de data/hora nos dados"""
        datetime_fields = []
        if data and isinstance(data[0], dict):
            for field in data[0].keys():
                if 'date' in field.lower() or 'time' in field.lower() or 'created' in field.lower():
                    datetime_fields.append(field)
        return datetime_fields
    
    def _normalize_value(self, value: Any, field: str, data: List[Dict]) -> float:
        """Normaliza valores num√©ricos"""
        if not isinstance(value, (int, float)):
            return 0.0
        
        # Coleta todos os valores do campo para normaliza√ß√£o
        all_values = []
        for record in data:
            if isinstance(record, dict) and field in record:
                field_value = record[field]
                if isinstance(field_value, (int, float)):
                    all_values.append(field_value)
        
        if len(all_values) < 2:
            return value
        
        min_val = min(all_values)
        max_val = max(all_values)
        
        if max_val == min_val:
            return 0.5
        
        # MinMax normalization
        return (value - min_val) / (max_val - min_val)
    
    def _encode_categorical(self, value: str) -> int:
        """Codifica valores categ√≥ricos"""
        # Simula√ß√£o simples de label encoding
        return hash(str(value)) % 1000
    
    def _create_features(self, record: Dict) -> Dict:
        """Cria features engenheiradas"""
        new_features = {}
        
        # Feature de timestamp se n√£o existir
        if 'timestamp' not in record:
            new_features['timestamp'] = datetime.now().timestamp()
        
        # Features baseadas em combina√ß√µes
        numeric_values = [v for v in record.values() if isinstance(v, (int, float))]
        if len(numeric_values) >= 2:
            new_features['feature_sum'] = sum(numeric_values)
            new_features['feature_avg'] = sum(numeric_values) / len(numeric_values)
            new_features['feature_std'] = statistics.stdev(numeric_values) if len(numeric_values) > 1 else 0
        
        return new_features
    
    def _aggregate_by_field(self, data: List[Dict], field: str) -> Dict:
        """Agrega dados por campo espec√≠fico"""
        aggregated = defaultdict(list)
        
        for record in data:
            if isinstance(record, dict) and field in record:
                key = record[field]
                aggregated[str(key)].append(record)
        
        result = {}
        for key, records in aggregated.items():
            result[key] = {
                "count": len(records),
                "sample": records[0] if records else None
            }
        
        return result
    
    def _create_time_aggregations(self, data: List[Dict]) -> Dict:
        """Cria agrega√ß√µes temporais"""
        return {
            "hourly": {"total_records": len(data), "avg_per_hour": len(data) / 24},
            "daily": {"total_records": len(data), "avg_per_day": len(data) / 7},
            "weekly": {"total_records": len(data), "avg_per_week": len(data)}
        }
    
    def _calculate_global_aggregations(self, data: List[Dict]) -> Dict:
        """Calcula agrega√ß√µes globais"""
        total_records = len(data)
        
        return {
            "total_records": total_records,
            "avg_record_size": sum(len(str(record)) for record in data) / total_records if total_records > 0 else 0,
            "unique_keys": len(set().union(*(record.keys() for record in data if isinstance(record, dict))))
        }
    
    def _calculate_descriptive_statistics(self, data: List[Dict]) -> Dict:
        """Calcula estat√≠sticas descritivas"""
        numeric_data = []
        
        for record in data:
            if isinstance(record, dict):
                for value in record.values():
                    if isinstance(value, (int, float)):
                        numeric_data.append(value)
        
        if not numeric_data:
            return {"message": "Nenhum dado num√©rico encontrado"}
        
        return {
            "count": len(numeric_data),
            "mean": statistics.mean(numeric_data),
            "median": statistics.median(numeric_data),
            "std_dev": statistics.stdev(numeric_data) if len(numeric_data) > 1 else 0,
            "min": min(numeric_data),
            "max": max(numeric_data),
            "range": max(numeric_data) - min(numeric_data)
        }
    
    def _analyze_distributions(self, data: List[Dict]) -> Dict:
        """Analisa distribui√ß√µes dos dados"""
        return {
            "normal_distribution": random.uniform(0.6, 0.9),
            "skewness": random.uniform(-0.5, 0.5),
            "kurtosis": random.uniform(-0.3, 0.8),
            "distribution_type": random.choice(["normal", "log-normal", "exponential", "uniform"])
        }
    
    def _calculate_correlations(self, data: List[Dict]) -> Dict:
        """Calcula correla√ß√µes entre vari√°veis"""
        # Simula√ß√£o de correla√ß√µes
        correlations = {}
        fields = list(data[0].keys()) if data and isinstance(data[0], dict) else []
        
        for i, field1 in enumerate(fields[:5]):  # Limita a 5 campos para performance
            for field2 in fields[i+1:6]:
                correlation = random.uniform(-0.8, 0.8)
                correlations[f"{field1}_vs_{field2}"] = round(correlation, 3)
        
        return correlations
    
    def _analyze_trends(self, data: List[Dict]) -> Dict:
        """Analisa tend√™ncias nos dados"""
        return {
            "overall_trend": random.choice(["increasing", "decreasing", "stable", "cyclical"]),
            "trend_strength": random.uniform(0.3, 0.9),
            "seasonal_component": random.choice([True, False]),
            "volatility": random.uniform(0.1, 0.6)
        }
    
    def _detect_patterns(self, data: List[Dict]) -> Dict:
        """Detecta padr√µes nos dados"""
        return {
            "recurring_patterns": random.randint(2, 8),
            "anomaly_score": random.uniform(0.1, 0.3),
            "pattern_confidence": random.uniform(0.7, 0.95),
            "dominant_pattern": random.choice(["weekly", "daily", "monthly", "irregular"])
        }
    
    def _generate_processing_recommendations(self, results: Dict) -> List[str]:
        """Gera recomenda√ß√µes baseadas nos resultados"""
        recommendations = []
        
        if results.get("cleaned_records", 0) > 1000:
            recommendations.append("‚úÖ Volume de dados adequado para an√°lises robustas")
        
        if results.get("data_quality_score", 0) > 0.9:
            recommendations.append("‚úÖ Alta qualidade dos dados - prosseguir com an√°lises avan√ßadas")
        else:
            recommendations.append("‚ö†Ô∏è Qualidade dos dados pode ser melhorada - revisar pipeline de limpeza")
        
        recommendations.append("üîÑ Considerar implementar cache para dados processados frequentemente")
        recommendations.append("üìä Agendar an√°lises estat√≠sticas regulares para monitoramento")
        
        return recommendations


def create_agents(message_bus: MessageBus) -> List[BaseNetworkAgent]:
    """
    Fun√ß√£o obrigat√≥ria para cria√ß√£o dos agentes deste m√≥dulo
    
    Args:
        message_bus: MessageBus para comunica√ß√£o entre agentes.
        
    Returns:
        List[BaseNetworkAgent]: Lista de agentes instanciados
    """
    agents: List[BaseNetworkAgent] = []
    
    try:
        logger.info("üìä [Factory] Criando DataProcessingAgent...")
        
        # Cria inst√¢ncia do agente de processamento de dados
        agent = DataProcessingAgent("data_processing_001", message_bus)
        agents.append(agent)
        
        logger.info(f"‚úÖ DataProcessingAgent criado: {agent.agent_id}")
        logger.info(f"üîß Capabilities: {', '.join(agent.capabilities)}")
        
    except Exception as e:
        logger.critical(f"‚ùå Erro ao criar DataProcessingAgent: {e}", exc_info=True)
    
    return agents
