/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ALSHAM QUANTUM - ORION VOICE HOOK (10/10 EDITION)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ğŸ“ PATH: frontend/src/hooks/useOrionVoice.ts
 * ğŸ¤ Toda lÃ³gica de voz: TTS (Text-to-Speech) + Speech Recognition
 * ğŸ’ Tratamento de erros elegante + Fallbacks silenciosos
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

import { useState, useRef, useCallback, useEffect } from 'react';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TYPES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Define o tipo para a API de Reconhecimento de Voz, que pode ter prefixos
type SpeechRecognitionType = typeof window.SpeechRecognition | typeof window.webkitSpeechRecognition | undefined;
const SpeechRecognition = (window.SpeechRecognition || window.webkitSpeechRecognition) as SpeechRecognitionType;

export interface VoiceState {
  isListening: boolean;
  isSpeaking: boolean;
  voiceEnabled: boolean;
  micPermissionDenied: boolean;
  browserSupported: boolean;
  voicesLoaded: boolean;
  currentTranscript: string;
  error: VoiceError | null;
}

export interface VoiceError {
  type: 'mic-denied' | 'mic-not-found' | 'browser-unsupported' | 'network' | 'tts-failed' | 'unknown';
  message: string;
  recoverable: boolean;
}

export interface UseOrionVoiceReturn {
  state: VoiceState;
  startListening: () => Promise<void>;
  stopListening: () => void;
  speak: (text: string) => void;
  cancelSpeech: () => void;
  toggleVoice: () => void;
  clearError: () => void;
  getMediaStream: () => MediaStream | null;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HOOK PRINCIPAL
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export function useOrionVoice(
  onTranscriptComplete?: (transcript: string) => void
): UseOrionVoiceReturn {
  // â•â•â• STATE â•â•â•
  const [state, setState] = useState<VoiceState>({
    isListening: false,
    isSpeaking: false,
    voiceEnabled: true,
    micPermissionDenied: false,
    browserSupported: false,
    voicesLoaded: false,
    currentTranscript: '',
    error: null,
  });

  // â•â•â• REFS â•â•â•
  // Usamos o tipo global SpeechRecognition que foi definido acima
  const recognitionRef = useRef<globalThis.SpeechRecognition | null>(null);
  const synthRef = useRef<SpeechSynthesis | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const onTranscriptCompleteRef = useRef(onTranscriptComplete);

  // Manter ref atualizada
  useEffect(() => {
    onTranscriptCompleteRef.current = onTranscriptComplete;
  }, [onTranscriptComplete]);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // INICIALIZAÃ‡ÃƒO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  useEffect(() => {
    if (typeof window === 'undefined') return;

    // Verificar suporte do navegador
    const browserSupported = !!SpeechRecognition && !!window.speechSynthesis;

    setState(prev => ({ ...prev, browserSupported }));

    if (!browserSupported) {
      console.warn('[ORION Voice] Navegador nÃ£o suporta Web Speech API');
      setState(prev => ({
        ...prev,
        error: {
          type: 'browser-unsupported',
          message: 'Seu navegador nÃ£o suporta reconhecimento de voz. Use Chrome, Edge ou Safari.',
          recoverable: false,
        },
      }));
      return;
    }

    // Inicializar sÃ­ntese de voz
    synthRef.current = window.speechSynthesis;

    const loadVoices = () => {
      const voices = synthRef.current?.getVoices();
      if (voices && voices.length > 0) {
        setState(prev => ({ ...prev, voicesLoaded: true }));
        console.log('[ORION Voice] Vozes carregadas:', voices.length);
      }
    };

    loadVoices();
    if (synthRef.current) {
      synthRef.current.onvoiceschanged = loadVoices;
    }

    // Cleanup
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      if (synthRef.current) {
        synthRef.current.cancel();
      }
      // Garante que o stream do microfone seja parado
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;
      }
    };
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // CLEAR ERROR
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const clearError = useCallback(() => {
    setState(prev => ({ ...prev, error: null, micPermissionDenied: false }));
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // TEXT-TO-SPEECH (VOZ J.A.R.V.I.S.)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const speak = useCallback((text: string) => {
    if (!synthRef.current || !state.voiceEnabled) {
      // Fallback silencioso
      return;
    }

    try {
      // Cancelar fala anterior
      synthRef.current.cancel();

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'pt-BR';
      utterance.rate = 0.92; // Velocidade elegante
      utterance.pitch = 0.75; // Voz grave J.A.R.V.I.S.
      utterance.volume = 1.0;

      // Encontrar melhor voz masculina brasileira
      const voices = synthRef.current.getVoices();
      const voicePriorities = [
        'Google portuguÃªs do Brasil',
        'Microsoft Daniel',
        'Daniel',
        'Luciano',
        'pt-BR-Wavenet-B',
        'pt-BR-Standard-B',
      ];

      let selectedVoice = voices.find(v =>
        voicePriorities.some(prio => v.name.toLowerCase().includes(prio.toLowerCase()))
      );

      if (!selectedVoice) {
        selectedVoice = voices.find(v => v.lang.includes('pt-BR'));
      }
      if (!selectedVoice) {
        selectedVoice = voices.find(v => v.lang.includes('pt'));
      }
      if (!selectedVoice && voices.length > 0) {
        selectedVoice = voices[0];
      }

      if (selectedVoice) {
        utterance.voice = selectedVoice;
      }

      utterance.onstart = () => {
        setState(prev => ({ ...prev, isSpeaking: true }));
      };

      utterance.onend = () => {
        setState(prev => ({ ...prev, isSpeaking: false }));
      };

      utterance.onerror = () => {
        // Fallback silencioso - nÃ£o mostrar erro para TTS
        setState(prev => ({ ...prev, isSpeaking: false }));
      };

      synthRef.current.speak(utterance);
    } catch (error) {
      // Fallback silencioso
      setState(prev => ({ ...prev, isSpeaking: false }));
    }
  }, [state.voiceEnabled]);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // CANCELAR FALA
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const cancelSpeech = useCallback(() => {
    if (synthRef.current) {
      synthRef.current.cancel();
      setState(prev => ({ ...prev, isSpeaking: false }));
    }
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // TOGGLE VOICE
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const toggleVoice = useCallback(() => {
    setState(prev => {
      if (prev.voiceEnabled && synthRef.current) {
        synthRef.current.cancel();
      }
      return { ...prev, voiceEnabled: !prev.voiceEnabled };
    });
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // INICIAR RECONHECIMENTO DE VOZ
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const startListening = useCallback(async () => {
    if (!state.browserSupported) {
      // O erro jÃ¡ foi setado na inicializaÃ§Ã£o
      return;
    }

    // Parar TTS se estiver falando
    if (synthRef.current) {
      synthRef.current.cancel();
    }

    // Limpar transcriÃ§Ã£o anterior
    setState(prev => ({ ...prev, currentTranscript: '', error: null, micPermissionDenied: false }));

    try {
      // 1. Pedir permissÃ£o de microfone e obter stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      mediaStreamRef.current = stream;

      // 2. Inicializar Speech Recognition
      if (!SpeechRecognition) {
        // NÃ£o deveria acontecer se browserSupported for true, mas Ã© um bom fallback
        throw new Error('Speech Recognition API not available.');
      }

      const recognition = new SpeechRecognition();
      recognition.continuous = false; // NÃ£o contÃ­nuo, para uma Ãºnica frase
      recognition.interimResults = true; // Resultados parciais para feedback visual
      recognition.lang = 'pt-BR';
      recognitionRef.current = recognition;

      // 3. Configurar Eventos
      recognition.onstart = () => {
        setState(prev => ({ ...prev, isListening: true }));
      };

      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        setState(prev => ({
          ...prev,
          currentTranscript: finalTranscript || interimTranscript,
        }));

        if (finalTranscript) {
          recognition.stop(); // Parar apÃ³s resultado final
        }
      };

      recognition.onerror = (event) => {
        console.error('[ORION Voice] Recognition Error:', event.error);
        setState(prev => ({ ...prev, isListening: false }));

        if (event.error === 'no-speech' || event.error === 'audio-capture') {
          // Erros recuperÃ¡veis
          setState(prev => ({
            ...prev,
            error: {
              type: 'mic-not-found',
              message: 'NÃ£o detectei sua voz. Tente novamente.',
              recoverable: true,
            },
          }));
        } else if (event.error === 'network') {
          setState(prev => ({
            ...prev,
            error: {
              type: 'network',
              message: 'Problema de conexÃ£o. Verifique sua internet.',
              recoverable: true,
            },
          }));
        } else {
          setState(prev => ({
            ...prev,
            error: {
              type: 'unknown',
              message: `Erro desconhecido: ${event.error}`,
              recoverable: true,
            },
          }));
        }
      };

      recognition.onend = () => {
        setState(prev => ({ ...prev, isListening: false }));
        const finalTranscript = recognitionRef.current?.currentTranscript || state.currentTranscript;

        // Se houver transcriÃ§Ã£o final, chamar o callback
        if (finalTranscript && onTranscriptCompleteRef.current) {
          onTranscriptCompleteRef.current(finalTranscript);
        }
        
        // Limpar o stream do microfone apÃ³s o uso
        if (mediaStreamRef.current) {
            mediaStreamRef.current.getTracks().forEach(track => track.stop());
            mediaStreamRef.current = null;
        }
      };

      // 4. Iniciar
      recognition.start();
      setState(prev => ({ ...prev, isListening: true }));

    } catch (error) {
      console.error('[ORION Voice] Microfone Error:', error);
      setState(prev => ({ ...prev, isListening: false }));

      if (error instanceof DOMException && error.name === 'NotAllowedError') {
        // PermissÃ£o negada pelo usuÃ¡rio
        setState(prev => ({
          ...prev,
          micPermissionDenied: true,
          error: {
            type: 'mic-denied',
            message: 'PermissÃ£o de microfone negada. Por favor, habilite-a nas configuraÃ§Ãµes do seu navegador.',
            recoverable: false,
          },
        }));
      } else {
        // Outros erros de microfone
        setState(prev => ({
          ...prev,
          error: {
            type: 'mic-not-found',
            message: 'NÃ£o foi possÃ­vel acessar o microfone. Verifique se ele estÃ¡ conectado.',
            recoverable: true,
          },
        }));
      }
      
      // Limpar o stream em caso de erro
      if (mediaStreamRef.current) {
          mediaStreamRef.current.getTracks().forEach(track => track.stop());
          mediaStreamRef.current = null;
      }
    }
  }, [state.browserSupported, state.currentTranscript]);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // PARAR RECONHECIMENTO DE VOZ
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      setState(prev => ({ ...prev, isListening: false }));
    }
    // Garante que o stream do microfone seja parado
    if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;
    }
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // GET MEDIA STREAM (Para o Visualizador de Ãudio)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const getMediaStream = useCallback(() => {
    return mediaStreamRef.current;
  }, []);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // RETURN
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  return {
    state,
    startListening,
    stopListening,
    speak,
    cancelSpeech,
    toggleVoice,
    clearError,
    getMediaStream,
  };
}
